{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOwaiShrWeyAIab0wUmvcHo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OmrySh/DecisionTransformer/blob/main/DecisionTransformersConnect4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F-DL9RGuecuO",
        "outputId": "c707e9c4-b91a-434a-a2d8-6e11cad1f212"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting open_spiel\n",
            "  Downloading open_spiel-1.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pip>=20.0.2 in /usr/local/lib/python3.10/dist-packages (from open_spiel) (23.1.2)\n",
            "Requirement already satisfied: attrs>=19.3.0 in /usr/local/lib/python3.10/dist-packages (from open_spiel) (23.1.0)\n",
            "Requirement already satisfied: absl-py>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from open_spiel) (1.4.0)\n",
            "Requirement already satisfied: numpy>=1.21.5 in /usr/local/lib/python3.10/dist-packages (from open_spiel) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.10.1 in /usr/local/lib/python3.10/dist-packages (from open_spiel) (1.10.1)\n",
            "Installing collected packages: open_spiel\n",
            "Successfully installed open_spiel-1.3\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade open_spiel"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "class DecisionTransformer(nn.Module):\n",
        "    def __init__(self, d_model, nhead, num_encoder_layers, num_decoder_layers, input_dim, action_dim, dropout=0.5):\n",
        "        super(DecisionTransformer, self).__init__()\n",
        "\n",
        "        # Define the transformer model\n",
        "        self.transformer = nn.Transformer(d_model, nhead, num_encoder_layers, num_decoder_layers, dropout=dropout)\n",
        "\n",
        "        # Define the input embedding layer\n",
        "        self.input_embedding = nn.Linear(input_dim+1, d_model)  # +1 for the reward\n",
        "\n",
        "        # Define the output action layer\n",
        "        self.output_action = nn.Linear(d_model, action_dim)\n",
        "\n",
        "    def forward(self, src, tgt):\n",
        "        print(f\"Source shape (before embedding): {src.shape}\")\n",
        "        print(f\"Target shape (before embedding): {tgt.shape}\")\n",
        "\n",
        "        src = self.input_embedding(src)\n",
        "        tgt = self.input_embedding(tgt)\n",
        "\n",
        "        print(f\"Source shape (after embedding): {src.shape}\")\n",
        "        print(f\"Target shape (after embedding): {tgt.shape}\")\n",
        "\n",
        "        transformer_output = self.transformer(src.unsqueeze(0), tgt.unsqueeze(0))\n",
        "        action_logits = self.output_action(transformer_output.squeeze(0))\n",
        "        return action_logits\n",
        "# # Testing the class instantiation\n",
        "# test_model = DecisionTransformer(D_MODEL, NHEAD, NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, INPUT_DIM, ACTION_DIM)\n",
        "# test_model\n"
      ],
      "metadata": {
        "id": "1S-tt06mD9u1"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "class DecisionTransformerModel:\n",
        "    def __init__(self, state_dim, action_dim, d_model, nhead, num_encoder_layers, num_decoder_layers):\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "        # Adjust the state dimension to account for the reward concatenated\n",
        "        self.model = DecisionTransformer(d_model, nhead, num_encoder_layers, num_decoder_layers,\n",
        "                                            state_dim, action_dim).to(self.device)\n",
        "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=0.001)\n",
        "        self.criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    def train(self, data, epochs, batch_size):\n",
        "        self.model.train()\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            np.random.shuffle(data)\n",
        "            batches = [data[i:i + batch_size] for i in range(0, len(data), batch_size)]\n",
        "            # dataloader = DataLoader(data, batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "\n",
        "            for batch in batches:\n",
        "                states, prev_actions, actions, rewards = zip(*batch)\n",
        "\n",
        "                # Reshape states to match the expected input shape for the model\n",
        "                states = torch.stack(states).view(batch_size, -1).to(self.device)\n",
        "                prev_actions = torch.stack([torch.tensor(pa) for pa in prev_actions]).to(self.device)  # Convert prev_actions to GPU tensor\n",
        "                actions = torch.stack([torch.tensor(a) for a in actions]).to(self.device)\n",
        "                rewards = torch.stack([torch.tensor(r) for r in rewards]).to(self.device)\n",
        "                if actions.shape[0] < 32:\n",
        "                    continue\n",
        "                # print(f\"Batch states shape: {states.shape}\")\n",
        "                # print(f\"Batch actions shape: {actions.shape}\")\n",
        "                # print(f\"Batch rewards shape: {rewards.shape}\")\n",
        "\n",
        "                # Concatenate the rewards to the states\n",
        "                src = torch.cat((states, prev_actions.unsqueeze(1), rewards.unsqueeze(1)), dim=1)  # Include prev_actions in concatenation\n",
        "                tgt = src.clone().detach()\n",
        "\n",
        "\n",
        "                print(f\"States shape: {states.shape}\")\n",
        "                print(f\"Actions shape: {actions.shape}\")\n",
        "                print(f\"Rewards shape: {rewards.shape}\")\n",
        "                print(f\"Source shape: {src.shape}\")\n",
        "                print(f\"Target shape: {tgt.shape}\")\n",
        "\n",
        "                # Forward pass\n",
        "                outputs = self.model(src, tgt)\n",
        "\n",
        "                # Compute loss\n",
        "                loss = self.criterion(outputs, actions)\n",
        "\n",
        "                # Backward pass and optimization\n",
        "                self.optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                self.optimizer.step()\n",
        "\n",
        "            print(f\"Epoch [{epoch + 1}/{epochs}], Loss: {loss.item():.4f}\")\n",
        "\n",
        "    def predict(self, state, prev_action, prev_reward):\n",
        "        self.model.eval()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            state = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
        "            prev_action = torch.LongTensor([prev_action]).unsqueeze(0).to(self.device)\n",
        "            prev_reward = torch.FloatTensor([prev_reward]).unsqueeze(0).to(self.device)\n",
        "\n",
        "            src = torch.cat((state, prev_action, prev_reward), dim=-1)\n",
        "            tgt = src.clone().detach()\n",
        "\n",
        "            print(f\"State shape: {state.shape}\")\n",
        "            print(f\"Prev_action shape: {prev_action.shape}\")\n",
        "            print(f\"Prev_reward shape: {prev_reward.shape}\")\n",
        "\n",
        "            src = torch.cat((state, prev_action, prev_reward), dim=-1)\n",
        "            tgt = src.clone().detach()\n",
        "\n",
        "            print(f\"Source shape (before model): {src.shape}\")\n",
        "            print(f\"Target shape (before model): {tgt.shape}\")\n",
        "\n",
        "            output = self.model(src, tgt)\n",
        "\n",
        "            predicted_action = torch.argmax(output, dim=-1).item()\n",
        "\n",
        "        return predicted_action\n"
      ],
      "metadata": {
        "id": "5ZJfbKXpEMvB"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pyspiel\n",
        "from open_spiel.python.algorithms import tabular_qlearner\n",
        "from open_spiel.python.rl_environment import Environment\n",
        "import torch\n",
        "\n",
        "def collect_data_fixed_length(num_games=1000, sequence_length=10):\n",
        "    \"\"\"Collects data of fixed sequence length from Q-learning players playing games.\"\"\"\n",
        "    game = pyspiel.load_game(\"connect_four\")\n",
        "    env = Environment(game)\n",
        "\n",
        "    # Initialize two Q-learning agents\n",
        "    players = [tabular_qlearner.QLearner(player_id=player_id, num_actions=env.action_spec()[\"num_actions\"])\n",
        "               for player_id in [0, 1]]\n",
        "\n",
        "    dataset = []\n",
        "\n",
        "    for _ in range(num_games):\n",
        "        time_step = env.reset()\n",
        "        game_data = []\n",
        "        prev_action = 0  # Initialize previous action as 0 for the first state\n",
        "\n",
        "        while not time_step.last():\n",
        "            current_player = time_step.observations[\"current_player\"]\n",
        "            state_tensor = torch.tensor(time_step.observations[\"info_state\"][current_player])\n",
        "            action = players[current_player].step(time_step).action\n",
        "            game_data.append((state_tensor, prev_action, action))\n",
        "            prev_action = action  # Update the previous action\n",
        "            time_step = env.step([action])\n",
        "\n",
        "        # Determine the reward\n",
        "        if time_step.rewards[0] == 1:  # Player 1 wins\n",
        "            reward = 1\n",
        "        elif time_step.rewards[0] == -1:  # Player 2 wins\n",
        "            reward = -1\n",
        "        else:  # Draw\n",
        "            reward = 0\n",
        "\n",
        "        # Ensure the game data is of fixed length\n",
        "        last_valid_state = game_data[-1][0] if game_data else torch.zeros_like(state_tensor)\n",
        "        last_valid_prev_action = game_data[-1][1] if game_data else 0\n",
        "        while len(game_data) < sequence_length:\n",
        "            game_data.append((last_valid_state, last_valid_prev_action, 0))\n",
        "\n",
        "        # If the game is longer than sequence_length, truncate it\n",
        "        game_data = game_data[:sequence_length]\n",
        "\n",
        "        # Attach the reward to each (state, action) tuple and add to the main dataset\n",
        "        game_data_with_rewards = [(s, pa, a, reward) for s, pa, a in game_data]\n",
        "        dataset.extend(game_data_with_rewards)\n",
        "\n",
        "    return dataset\n",
        "\n",
        "\n",
        "def check_data_shapes(data):\n",
        "    state_shapes = set()\n",
        "    prev_action_shapes = set()\n",
        "    action_shapes = set()\n",
        "    reward_shapes = set()\n",
        "\n",
        "    for state, prev_action, action, reward in data:\n",
        "        state_shapes.add(state.shape)\n",
        "        prev_action_shapes.add(np.array(prev_action).shape)\n",
        "        action_shapes.add(np.array(action).shape)\n",
        "        reward_shapes.add(np.array(reward).shape)\n",
        "\n",
        "    print(\"Unique state shapes:\", state_shapes)\n",
        "    print(\"Unique previous action shapes:\", prev_action_shapes)\n",
        "    print(\"Unique action shapes:\", action_shapes)\n",
        "    print(\"Unique reward shapes:\", reward_shapes)\n",
        "\n",
        "\n",
        "data = collect_data_fixed_length()\n",
        "check_data_shapes(data)\n",
        "\n",
        "# 2. Train the Decision Transformer\n",
        "\n",
        "# Hyperparameters\n",
        "D_MODEL = 512\n",
        "NHEAD = 8\n",
        "NUM_ENCODER_LAYERS = 6\n",
        "NUM_DECODER_LAYERS = 6\n",
        "INPUT_DIM = 127  # Connect 4 board has 7x6 = 42 positions\n",
        "ACTION_DIM = 7  # 7 possible columns to drop a piece\n",
        "EPOCHS = 1\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "data = collect_data_fixed_length()\n",
        "model = DecisionTransformerModel(INPUT_DIM, ACTION_DIM, D_MODEL, NHEAD, NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS)\n",
        "model.train(data, EPOCHS, BATCH_SIZE)\n",
        "\n",
        "# 3. Test the trained model\n",
        "\n",
        "def test_model(model, num_tests=100):\n",
        "    game = pyspiel.load_game(\"connect_four\")\n",
        "\n",
        "    correct_predictions = 0\n",
        "\n",
        "    for _ in range(num_tests):\n",
        "        state = game.new_initial_state().observation_tensor()\n",
        "        true_action = np.random.choice(7)  # Randomly choose among the 7 possible actions\n",
        "        predicted_action = model.predict(state, true_action, 0)  # Assume previous reward as 0 for simplicity\n",
        "\n",
        "        if true_action == predicted_action:\n",
        "            correct_predictions += 1\n",
        "\n",
        "    accuracy = correct_predictions / num_tests\n",
        "    print(f\"Accuracy on test data: {accuracy:.2f}\")\n",
        "\n",
        "test_model(model)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "9KZAcg-7DhhC",
        "outputId": "76bcf122-f34d-4030-84ea-49bcdedb2614"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-2349fc79d76f>\u001b[0m in \u001b[0;36m<cell line: 73>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcollect_data_fixed_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0mcheck_data_shapes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-2349fc79d76f>\u001b[0m in \u001b[0;36mcollect_data_fixed_length\u001b[0;34m(num_games, sequence_length)\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0mcurrent_player\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime_step\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"current_player\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0mstate_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_step\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"info_state\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcurrent_player\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcurrent_player\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m             \u001b[0mgame_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprev_action\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0mprev_action\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maction\u001b[0m  \u001b[0;31m# Update the previous action\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/open_spiel/python/algorithms/tabular_qlearner.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, time_step, is_evaluation)\u001b[0m\n\u001b[1;32m    119\u001b[0m     \u001b[0;31m# Learn step: don't learn during evaluation or at first agent steps.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prev_info_state\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_evaluation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m       \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime_step\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_player_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtime_step\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Q values are zero for terminal.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         target += self._discount_factor * max(\n",
            "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
          ]
        }
      ]
    }
  ]
}